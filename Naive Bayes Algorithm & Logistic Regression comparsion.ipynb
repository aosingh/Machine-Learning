{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import chain, groupby\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, df, output_col_name, features):\n",
    "        self.df = df\n",
    "        self.conditional_probabilites = {}\n",
    "        self.prior_probabilites = {}\n",
    "        self.output = output_col_name\n",
    "        self.features = features\n",
    "        \n",
    "    def calculate_prior_probabilites(self):\n",
    "        groups = self.df.groupby(self.output).groups\n",
    "        for x in groups:\n",
    "            self.prior_probabilites[x] = len(groups[x])/len(df)\n",
    "    \n",
    "    def calculate_conditional_probabilites(self):\n",
    "        for x in self.features:\n",
    "            if x != self.output:\n",
    "                self.conditional_probabilites[x] = self.df.groupby(self.output)[x].value_counts()/self.df.groupby(self.output)[x].count()\n",
    "                \n",
    "    def train(self):\n",
    "        self.calculate_prior_probabilites()\n",
    "        self.calculate_conditional_probabilites()\n",
    "    \n",
    "    def predict(self, row):\n",
    "        posterior_prob = {}\n",
    "        \n",
    "        # Initialize the posterior probability as same as the priors\n",
    "        for x in self.prior_probabilites:\n",
    "            posterior_prob[x] = self.prior_probabilites[x]\n",
    "        \n",
    "        for label in posterior_prob:\n",
    "            for x in self.features:\n",
    "                if x != self.output:\n",
    "                    if label in self.conditional_probabilites[x] and row[x] in self.conditional_probabilites[x].get(label):\n",
    "                        posterior_prob[label] *= self.conditional_probabilites[x].get(label).get(row[x])\n",
    "                    else:\n",
    "                        posterior_prob[label] = 0\n",
    "                        break\n",
    "        return max(posterior_prob, key=posterior_prob.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, X, output, w, alpha):\n",
    "        self.X = X\n",
    "        self.output = output\n",
    "        self.alpha = alpha\n",
    "        self.w = w\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_function(x):\n",
    "        val = 1/(1 + np.exp(-x))\n",
    "        return val\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_regressor(w, X):\n",
    "        regressor = np.dot(w, X)\n",
    "        return LogisticRegression.sigmoid_function(regressor)\n",
    "       \n",
    "    def gradient(self, j):\n",
    "        total_error = 0\n",
    "        for i in xrange(len(self.output)):\n",
    "            xi = self.X[i]\n",
    "            xij = xi[j]\n",
    "            yi = self.output[i]\n",
    "            ri = LogisticRegression.compute_regressor(self.w, xi)\n",
    "            if yi == -1:\n",
    "                yi = 0\n",
    "            diff = xij*(yi-ri)\n",
    "            total_error += diff\n",
    "        constant = float(self.alpha)/float(len(self.output))\n",
    "        grad = constant * total_error\n",
    "        return grad\n",
    "    \n",
    "    \n",
    "    def gradient_ascent(self):\n",
    "        wi = []\n",
    "        for j in xrange(len(self.w)):\n",
    "            grad = self.gradient(j)\n",
    "            wij = self.w[j] + grad[0]\n",
    "            wi.append(wij)\n",
    "        return wi\n",
    "    \n",
    "    \n",
    "    def train(self, max_iters):\n",
    "        for counter in xrange(max_iters):\n",
    "            wi = self.gradient_ascent()\n",
    "            self.w = wi;\n",
    "    \n",
    "    def predict(self, x):\n",
    "        posterior_prob = LogisticRegression.compute_regressor(self.w, x)\n",
    "        if posterior_prob > 0.5:\n",
    "            return 1\n",
    "        elif posterior_prob < 0.5:\n",
    "            return -1\n",
    "        else:\n",
    "            return random.choice([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "x1        0\n",
       "x2        0\n",
       "x3        0\n",
       "x4        0\n",
       "x5        0\n",
       "x6        0\n",
       "x7        0\n",
       "x8        0\n",
       "x9        0\n",
       "output    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPath = './DataSets/breast-cancer-wisconsin.data.txt'\n",
    "columns = ['id', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'output']\n",
    "features = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'output']\n",
    "df = pd.read_csv(dataPath, names=columns)\n",
    "df = df.replace('?', np.nan)\n",
    "df[['x6']] = df[['x6']].apply(pd.to_numeric)\n",
    "df = df.replace(np.nan, df['x6'].mean())\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['output'] = df['output'].apply(lambda x : 1 if x == 2 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468\n",
      "468\n"
     ]
    }
   ],
   "source": [
    "df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df[features], df['output'], \n",
    "                                                                test_size=0.33, random_state=42)\n",
    "print len(df_X_train)\n",
    "print len(df_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66.666666666666671, 66.666666666666671, 66.666666666666671, 71.428571428571431, 96.969696969696969, 96.969696969696969]\n"
     ]
    }
   ],
   "source": [
    "fractions = [0.01, 0.02, 0.03, 0.125, 0.625, 1]\n",
    "training_data_size = []\n",
    "nb_accuracy = []\n",
    "\n",
    "for fraction in fractions:\n",
    "    df_train = df_X_train.sample(frac=fraction)\n",
    "    training_data_size.append(len(df_train))\n",
    "    \n",
    "    misclassfication_total = 0\n",
    "    avg_val = 0\n",
    "    \n",
    "    for counter in range(1, 6, 1):\n",
    "        nbclassifier = NaiveBayesClassifier(df=df_train, output_col_name='output', features=features)\n",
    "        nbclassifier.train()\n",
    "        df_X_test['prediction'] = df_X_test.apply(lambda row : nbclassifier.predict(row), axis=1)\n",
    "        accuracy_results = pd.crosstab(df_X_test['prediction'], df_y_test)\n",
    "\n",
    "        misclassifcation = 0;\n",
    "\n",
    "        if 1 in accuracy_results:\n",
    "            if -1 in accuracy_results[1]:\n",
    "                misclassifcation += accuracy_results[1][-1]\n",
    "\n",
    "\n",
    "        if -1 in accuracy_results:\n",
    "            if 1 in accuracy_results[-1]:\n",
    "                misclassifcation += accuracy_results[-1][1]\n",
    "\n",
    "        misclassfication_total += misclassifcation\n",
    "    avg_val = misclassfication_total/5\n",
    "    \n",
    "    avg_val = 100 -((avg_val/len(df_X_test))*100)\n",
    "    \n",
    "    nb_accuracy.append(avg_val)\n",
    "print nb_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71.861471861471856, 76.623376623376629, 71.861471861471856, 77.922077922077918, 82.683982683982691, 82.251082251082252]\n"
     ]
    }
   ],
   "source": [
    "fractions = [0.01, 0.02, 0.03, 0.125, 0.625, 1]\n",
    "training_data_size = []\n",
    "cols = ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9']\n",
    "lr_accuracy = []\n",
    "for fraction in fractions:\n",
    "    \n",
    "    df_train = df_X_train.sample(frac=fraction)\n",
    "    training_data_size.append(len(df_train))\n",
    "    \n",
    "    misclassfication_total = 0\n",
    "    avg_val = 0\n",
    "    \n",
    "    for counter in range(1, 6, 1):\n",
    "        w = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "        lr = LogisticRegression(df_train.as_matrix(columns=cols), df_train.as_matrix(columns=['output']), w, 0.1)\n",
    "        lr.train(100)\n",
    "        #print lr.w\n",
    "        df_X_test['lprediction'] = df_X_test.apply(lambda row : lr.predict(row.as_matrix(columns=cols)), axis=1)\n",
    "        accuracy_results = pd.crosstab(df_X_test['lprediction'], df_y_test)\n",
    "        misclassifcation = 0;\n",
    "\n",
    "        if 1 in accuracy_results:\n",
    "            if -1 in accuracy_results[1]:\n",
    "                misclassifcation += accuracy_results[1][-1]\n",
    "\n",
    "\n",
    "        if -1 in accuracy_results:\n",
    "            if 1 in accuracy_results[-1]:\n",
    "                misclassifcation += accuracy_results[-1][1]\n",
    "\n",
    "        misclassfication_total += misclassifcation\n",
    "        \n",
    "    avg_val = misclassfication_total/5\n",
    "    \n",
    "    avg_val = 100 -((avg_val/len(df_X_test))*100)\n",
    "    \n",
    "    lr_accuracy.append(avg_val)\n",
    "print lr_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "ax.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
